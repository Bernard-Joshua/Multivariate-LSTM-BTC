{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:44.849892Z\",\"iopub.execute_input\":\"2023-02-11T02:05:44.850347Z\",\"iopub.status.idle\":\"2023-02-11T02:05:44.878606Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:44.850298Z\",\"shell.execute_reply\":\"2023-02-11T02:05:44.877544Z\"}}\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import Sequential # Deep learning library, used for neural networks\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout # Deep learning classes for recurrent and regular densely-connected layers\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint # EarlyStopping during model training\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.activations import relu\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import zscore\nimport warnings\n\nsns.set_theme(style=\"darkgrid\")\nwarnings.simplefilter('ignore', np.RankWarning)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# %% [markdown]\n# # Getting the Data and evaluating it.\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:44.883338Z\",\"iopub.execute_input\":\"2023-02-11T02:05:44.884748Z\",\"iopub.status.idle\":\"2023-02-11T02:05:44.980714Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:44.884711Z\",\"shell.execute_reply\":\"2023-02-11T02:05:44.979719Z\"}}\ndf = pd.read_csv(\"/kaggle/input/btcinusd/BTC-Hourly.csv\", index_col='date', parse_dates=True)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:44.982347Z\",\"iopub.execute_input\":\"2023-02-11T02:05:44.983038Z\",\"iopub.status.idle\":\"2023-02-11T02:05:45.009238Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:44.983000Z\",\"shell.execute_reply\":\"2023-02-11T02:05:45.008610Z\"}}\ndf = df.drop(['unix', 'symbol'], axis=1)\ndf = df.sort_index(ascending=True)\ndf.head()\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:45.013122Z\",\"iopub.execute_input\":\"2023-02-11T02:05:45.013465Z\",\"iopub.status.idle\":\"2023-02-11T02:05:45.032090Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:45.013430Z\",\"shell.execute_reply\":\"2023-02-11T02:05:45.031075Z\"}}\ndf.info()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:45.035749Z\",\"iopub.execute_input\":\"2023-02-11T02:05:45.036241Z\",\"iopub.status.idle\":\"2023-02-11T02:05:45.051650Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:45.036207Z\",\"shell.execute_reply\":\"2023-02-11T02:05:45.048851Z\"}}\ndf.isnull().sum()\n\n# %% [markdown]\n# * **No null values.**\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:45.053135Z\",\"iopub.execute_input\":\"2023-02-11T02:05:45.053800Z\",\"iopub.status.idle\":\"2023-02-11T02:05:45.097493Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:45.053761Z\",\"shell.execute_reply\":\"2023-02-11T02:05:45.096438Z\"}}\ndf.describe()\n\n# %% [markdown]\n# *** Summary of the data. I can be seen that the Open, High, Low and Close values are very much simlar based on the mean and standard deviation.**\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:45.099243Z\",\"iopub.execute_input\":\"2023-02-11T02:05:45.100009Z\",\"iopub.status.idle\":\"2023-02-11T02:05:45.112784Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:45.099969Z\",\"shell.execute_reply\":\"2023-02-11T02:05:45.111368Z\"}}\ndf[df['Volume BTC'] == 0].count() # Get all the count with values that are 0.\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:45.114488Z\",\"iopub.execute_input\":\"2023-02-11T02:05:45.115177Z\",\"iopub.status.idle\":\"2023-02-11T02:05:45.159718Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:45.115139Z\",\"shell.execute_reply\":\"2023-02-11T02:05:45.158655Z\"}}\n# Remove zero values ( They act as outliers that my affect the models accuracy )\ndf = df[df['Volume BTC'] != 0]\ndf.describe()\n\n# %% [markdown]\n# # Univariate Analysis\n# Checking distributions for more outliers and skewedness.\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:45.163484Z\",\"iopub.execute_input\":\"2023-02-11T02:05:45.163917Z\",\"iopub.status.idle\":\"2023-02-11T02:05:45.185771Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:45.163882Z\",\"shell.execute_reply\":\"2023-02-11T02:05:45.184849Z\"}}\ndx = df.copy()\ndx = dx.apply(zscore) # Application of Z-Score helps determine values that are outliers ( More than 3 STD from the mean )\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:45.193758Z\",\"iopub.execute_input\":\"2023-02-11T02:05:45.194845Z\",\"iopub.status.idle\":\"2023-02-11T02:05:45.494447Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:45.194803Z\",\"shell.execute_reply\":\"2023-02-11T02:05:45.493468Z\"}}\nsns.histplot(data=dx, x=dx['close'], stat='count')\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:45.495725Z\",\"iopub.execute_input\":\"2023-02-11T02:05:45.496562Z\",\"iopub.status.idle\":\"2023-02-11T02:05:45.794504Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:45.496524Z\",\"shell.execute_reply\":\"2023-02-11T02:05:45.793498Z\"}}\nsns.histplot(data=dx, x=dx['open'], stat='count')\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:45.797520Z\",\"iopub.execute_input\":\"2023-02-11T02:05:45.797829Z\",\"iopub.status.idle\":\"2023-02-11T02:05:46.090604Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:45.797801Z\",\"shell.execute_reply\":\"2023-02-11T02:05:46.089489Z\"}}\nsns.histplot(data=dx, x=dx['high'], stat='count')\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:46.092007Z\",\"iopub.execute_input\":\"2023-02-11T02:05:46.092904Z\",\"iopub.status.idle\":\"2023-02-11T02:05:46.385535Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:46.092865Z\",\"shell.execute_reply\":\"2023-02-11T02:05:46.384635Z\"}}\nsns.histplot(data=dx, x=dx['low'], stat='count')\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:46.386851Z\",\"iopub.execute_input\":\"2023-02-11T02:05:46.387302Z\",\"iopub.status.idle\":\"2023-02-11T02:05:46.650520Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:46.387265Z\",\"shell.execute_reply\":\"2023-02-11T02:05:46.649613Z\"}}\nplt.figure(figsize=(16,16))\nsns.boxplot(data=df, x=df['Volume USD'])\nplt.show()\n\n# %% [markdown]\n# * The values for the Open, High, Low and Close all seem to have similar distributions and positive skewness.\n# * No values from the Open, High, Low and Close fall more than 3 std from the mean, and were reported at a confidence interval of 97.4%.\n# * The Volumes both have extreme outliers. They are positively skewed.\n\n# %% [markdown]\n# # Bivariate Analysis\n# * Determining the correlations of the features with the targets. High correlation may indicate a relationship which, could indicate that they are good candidates as features for the models.\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:46.652404Z\",\"iopub.execute_input\":\"2023-02-11T02:05:46.653425Z\",\"iopub.status.idle\":\"2023-02-11T02:05:47.032503Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:46.653387Z\",\"shell.execute_reply\":\"2023-02-11T02:05:47.031607Z\"}}\nplt.figure(figsize=(10,10))\nsns.heatmap(df.corr(), annot=True, linewidth=0.5)\nplt.show()\n\n# %% [markdown]\n# * Seems like there is high correlations of the High,Open and Low with the Close. ( Might need a PCA )\n# * There seems to be a low negative correlation between the close and the Volume BTC ( Needs more research )\n# * The Volume USD seems to have a moderate correlation to the target (Close).\n\n# %% [markdown]\n# # Principal Component Analysis\n# * Remove multicolinearity among the High,Low and Open, features.\n# * Decorrelate the High,Low and Open, features with the Close (Target).\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:47.034415Z\",\"iopub.execute_input\":\"2023-02-11T02:05:47.035470Z\",\"iopub.status.idle\":\"2023-02-11T02:05:47.058981Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:47.035430Z\",\"shell.execute_reply\":\"2023-02-11T02:05:47.057942Z\"}}\nfeatures = df[['high','open','low','close']]\nfeatures =  (features - features.mean(axis=0)) / features.std(axis=0)\n\nscaler = MinMaxScaler()\n\npca = PCA()\nfeatures_pca = pca.fit_transform(features)\n\ncomponent_names = [f\"PC{i+1}\" for i in range(features_pca.shape[1])]\nfeatures_pca = pd.DataFrame(features_pca, columns=component_names)\n\nfeatures_pca.head()\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:47.060492Z\",\"iopub.execute_input\":\"2023-02-11T02:05:47.060942Z\",\"iopub.status.idle\":\"2023-02-11T02:05:47.074966Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:47.060908Z\",\"shell.execute_reply\":\"2023-02-11T02:05:47.073697Z\"}}\nloadings = pd.DataFrame(\n    pca.components_.T,  # transpose the matrix of loadings\n    columns=component_names,  # so the columns are the principal components\n    index=features.columns,  # and the rows are the original features\n)\nloadings\n\n# %% [markdown]\n# * The High, Open and close values played an important role in seperating the clusters. Which indicates that the most variance in the data came from these variables.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:47.076441Z\",\"iopub.execute_input\":\"2023-02-11T02:05:47.076907Z\",\"iopub.status.idle\":\"2023-02-11T02:05:47.305856Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:47.076868Z\",\"shell.execute_reply\":\"2023-02-11T02:05:47.304757Z\"}}\nimport matplotlib.pyplot as plt\nPC_values = np.arange(pca.n_components_) + 1\nplt.plot(PC_values, pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\nplt.title('Scree Plot')\nplt.xlabel('Principal Component')\nplt.ylabel('Variance Explained')\nplt.show()\n\nprint(np.round(pca.explained_variance_ratio_))\n\n\n# %% [markdown]\n# * PC1 makes up all of the variance in the data set the specific variation was around 99 %.\n# * This means that a combination of the Open, High and Close manage to capture the pattern in the data well.\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:47.307761Z\",\"iopub.execute_input\":\"2023-02-11T02:05:47.308425Z\",\"iopub.status.idle\":\"2023-02-11T02:05:47.315908Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:47.308388Z\",\"shell.execute_reply\":\"2023-02-11T02:05:47.314951Z\"}}\ndx = df.copy()\ndx = dx.reset_index()\ndx = dx.join(features_pca)\n\n\n# %% [markdown]\n# # Selecting the Best Features from the Dataset \n# * Done using a RandomForest Regressor and Entrophy method.\n# * Lower entrophy better.\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:47.317645Z\",\"iopub.execute_input\":\"2023-02-11T02:05:47.318064Z\",\"iopub.status.idle\":\"2023-02-11T02:05:48.677542Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:47.318029Z\",\"shell.execute_reply\":\"2023-02-11T02:05:48.676551Z\"}}\nfeatures_x = dx.drop(['date','close'], axis=1)\ntarget = dx.pop('close')\n\nd_features = features_x.dtypes == int\n\nmi_scores = mutual_info_regression(features_x, target, discrete_features=d_features)\nmi_scores =  pd.Series(mi_scores, name=\"MI Scores\", index=features_x.columns)\nmi_scores = mi_scores.sort_values(ascending=False)\n\nmi_scores\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:48.679141Z\",\"iopub.execute_input\":\"2023-02-11T02:05:48.679530Z\",\"iopub.status.idle\":\"2023-02-11T02:05:48.688910Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:48.679493Z\",\"shell.execute_reply\":\"2023-02-11T02:05:48.687958Z\"}}\nmis = mi_scores.to_frame()\nmis\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:48.690153Z\",\"iopub.execute_input\":\"2023-02-11T02:05:48.691036Z\",\"iopub.status.idle\":\"2023-02-11T02:05:48.936098Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:48.691000Z\",\"shell.execute_reply\":\"2023-02-11T02:05:48.935204Z\"}}\nsns.barplot(data=mis, x=mis['MI Scores'], y=mis.index)\n\n# %% [markdown]\n# **Only taking the Top 3 features with the highest information gain.**\n# * PC1\n# * High\n# * Low\n\n# %% [markdown]\n# * By theory PC1 should be enough to help the model learn the pattern. But follwing best practices to improve accuracy of a model it should be fitted with more that one variable. So, the top 3 variables with the most information gain is used.\n\n# %% [markdown]\n# # Building the LSTM Model\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:48.937404Z\",\"iopub.execute_input\":\"2023-02-11T02:05:48.938130Z\",\"iopub.status.idle\":\"2023-02-11T02:05:48.952473Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:48.938093Z\",\"shell.execute_reply\":\"2023-02-11T02:05:48.951613Z\"}}\n#Preparing the data ( Remove all unwanted features)\nbtc_features = dx[['PC1', 'high', 'low']]\nbtc_target = target\n\nbtc_dates =  dx.pop('date')\n\n# Feature Scaler \nf_scaler = MinMaxScaler()\n\n# Target Scaler\nt_scaler = MinMaxScaler() # The t_scaler will be used to insverse the transformation of 1-D data later ( output ). This is because the scalers are only able to transform data back to their original values based on the fitted dimension.\n\n# Convert to numpy Array\nbtc_features = btc_features.to_numpy()\nbtc_target = btc_target.to_numpy()\nbtc_dates = btc_dates.to_numpy()\n# Normalize\nbtc_features = f_scaler.fit_transform(btc_features)\nbtc_target = t_scaler.fit_transform(btc_target.reshape(-1,1))\n\nprint(btc_dates)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:48.954279Z\",\"iopub.execute_input\":\"2023-02-11T02:05:48.955012Z\",\"iopub.status.idle\":\"2023-02-11T02:05:48.993100Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:48.954975Z\",\"shell.execute_reply\":\"2023-02-11T02:05:48.992103Z\"}}\n# Using for loops to reshape the data to a 24-hour rolling window. The logic behind this is to use data from the previous 24-hrs to predict the 25th hour.\nx = [] # The features needed for prediction.\ny = [] # The expected result.\n\ndate = []\ny_date = []\n\nfor i in range(len(btc_features)-24):\n    x.append(btc_features[i:i+24])\n\nfor i in range(len(btc_target)-24):\n        y.append(btc_target[i])\n        \n# Used to verify if batches are indeed shuffled by batch and not by row.       \n\"\"\"\nfor i in range(len(btc_dates)-24):\n    date.append(btc_dates[i:i+24])\n    \nfor i in range(len(btc_dates)-24):\n        y_date.append(btc_dates[i])\n\"\"\"\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:48.994533Z\",\"iopub.execute_input\":\"2023-02-11T02:05:48.994886Z\",\"iopub.status.idle\":\"2023-02-11T02:05:49.036183Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:48.994853Z\",\"shell.execute_reply\":\"2023-02-11T02:05:49.035058Z\"}}\n# Validating that the data is in 3-Dimenstions. Neural-Networks need 3-D data.\nX = np.array(x)\nY = np.array(y)\nDate = np.array(date)\nY_date = np.array(y_date)\n\nprint(X.shape)\nprint(Y.shape)\n\n# Used to verify if batches are indeed shuffled by batch and not by row. \n# print(Date.shape)\n# print(Y_date.shape)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:49.037874Z\",\"iopub.execute_input\":\"2023-02-11T02:05:49.038245Z\",\"iopub.status.idle\":\"2023-02-11T02:05:49.057178Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:49.038210Z\",\"shell.execute_reply\":\"2023-02-11T02:05:49.055072Z\"}}\n# Spliting the data to 80% training and 20% testing. By hyperparameter tuning the training data will be 70% and 10% will be reserved for validation.\nX_train, x_test, Y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle=True) # Conduct batch shuffling\n\n# Used to verify if batches are indeed shuffled by batch and not by row.   \n#D_train, d_test, YD_train, yd_test = train_test_split(Date, Y_date, test_size=0.2, shuffle=True) # Conduct batch shuffling\n\nprint(X_train.shape)\nprint(Y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)\n\"\"\"\nprint(\"-----------------\")\nprint(D_train.shape)\nprint(YD_train.shape)\nprint(d_test.shape)\nprint(yd_test.shape)\n\"\"\"\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:49.058347Z\",\"iopub.execute_input\":\"2023-02-11T02:05:49.058662Z\",\"iopub.status.idle\":\"2023-02-11T02:05:49.063588Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:49.058626Z\",\"shell.execute_reply\":\"2023-02-11T02:05:49.062531Z\"}}\n# Used to verify if batches are indeed shuffled by batch and not by row.\n# print(D_train)\n# print(YD_train)\n\n# %% [markdown]\n# * Cypto data has high noise due to the volatility of crypto currencies, hence the dropout feature is used.\n# * Regularization process of dropout makes the neurons independent of each other so that all of them can perform better with less noise.\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:49.070317Z\",\"iopub.execute_input\":\"2023-02-11T02:05:49.071129Z\",\"iopub.status.idle\":\"2023-02-11T02:05:51.164504Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:49.071094Z\",\"shell.execute_reply\":\"2023-02-11T02:05:51.163561Z\"}}\nmodel = Sequential()\nmodel.add(LSTM(400, return_sequences=True, input_shape=(24,3)))\nmodel.add(LSTM(350, return_sequences=True, dropout=0.5)) # Dropout of 0.5 determined trough trial and error.\nmodel.add(LSTM(300, return_sequences=True, dropout=0.5))\nmodel.add(LSTM(350, return_sequences=True, dropout=0.5))\nmodel.add(LSTM(200, return_sequences=True, dropout=0.5))\nmodel.add(LSTM(350, return_sequences=True, dropout=0.5))\nmodel.add(LSTM(50, return_sequences=False))\nmodel.add(Dense(25, activation='relu'))    # ReLu is used as the data has a linear trend ( Based on the residual plot of the target)\nmodel.add(Dense(20,  activation='relu'))\nmodel.add(Dense(15,  activation='relu'))\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dense(5,  activation='relu'))\nmodel.add(Dense(1))\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:51.165960Z\",\"iopub.execute_input\":\"2023-02-11T02:05:51.166651Z\",\"iopub.status.idle\":\"2023-02-11T02:05:51.174170Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:51.166614Z\",\"shell.execute_reply\":\"2023-02-11T02:05:51.173249Z\"}}\n#  Early stopping callback used to prevent overfitting.\nearly_stopping = EarlyStopping(\n    patience=5, # Minimum cycles of no signal learnt by the model to stop the learning process.\n    min_delta=0.001, # Minimum learning required to continue.\n    verbose=1, # Debug messages set to True\n    restore_best_weights=True, # Restore model weights from the epoch with the best value of the monitored quantity. \n)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:51.175407Z\",\"iopub.execute_input\":\"2023-02-11T02:05:51.175991Z\",\"iopub.status.idle\":\"2023-02-11T02:05:51.202526Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:51.175958Z\",\"shell.execute_reply\":\"2023-02-11T02:05:51.201464Z\"}}\nmodel.compile(optimizer='adam', loss='mae', metrics=['mean_absolute_percentage_error'] ) # Compiling using the Stohastic Gradient Decsent algorithm, Adam and loss montoring function of Mean Absolute Error as the model is performing a regression.\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:05:51.204058Z\",\"iopub.execute_input\":\"2023-02-11T02:05:51.206450Z\",\"iopub.status.idle\":\"2023-02-11T02:06:53.148277Z\",\"shell.execute_reply.started\":\"2023-02-11T02:05:51.206416Z\",\"shell.execute_reply\":\"2023-02-11T02:06:53.147309Z\"}}\nhistory = model.fit(X_train, Y_train, batch_size=2000, epochs=15, callbacks=early_stopping, validation_split=0.1, verbose=0)\n\n# %% [markdown]\n# * **Batch Size and Epoch determined by trial and error by evaluating training and validation loss**\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:06:53.149939Z\",\"iopub.execute_input\":\"2023-02-11T02:06:53.150325Z\",\"iopub.status.idle\":\"2023-02-11T02:06:53.480072Z\",\"shell.execute_reply.started\":\"2023-02-11T02:06:53.150286Z\",\"shell.execute_reply\":\"2023-02-11T02:06:53.479156Z\"}}\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:06:53.484227Z\",\"iopub.execute_input\":\"2023-02-11T02:06:53.486436Z\",\"iopub.status.idle\":\"2023-02-11T02:06:57.123632Z\",\"shell.execute_reply.started\":\"2023-02-11T02:06:53.486399Z\",\"shell.execute_reply\":\"2023-02-11T02:06:57.122631Z\"}}\nprediction = model.predict(x_test)\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:06:57.125208Z\",\"iopub.execute_input\":\"2023-02-11T02:06:57.125748Z\",\"iopub.status.idle\":\"2023-02-11T02:06:57.132214Z\",\"shell.execute_reply.started\":\"2023-02-11T02:06:57.125710Z\",\"shell.execute_reply\":\"2023-02-11T02:06:57.131295Z\"}}\nprediction = t_scaler.inverse_transform(prediction) # Inversing the predictions from the normalized values\nactual = t_scaler.inverse_transform(y_test)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:06:57.133422Z\",\"iopub.execute_input\":\"2023-02-11T02:06:57.134218Z\",\"iopub.status.idle\":\"2023-02-11T02:06:57.146086Z\",\"shell.execute_reply.started\":\"2023-02-11T02:06:57.134181Z\",\"shell.execute_reply\":\"2023-02-11T02:06:57.144991Z\"}}\nprediction.shape\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:06:57.147868Z\",\"iopub.execute_input\":\"2023-02-11T02:06:57.148260Z\",\"iopub.status.idle\":\"2023-02-11T02:06:57.155906Z\",\"shell.execute_reply.started\":\"2023-02-11T02:06:57.148225Z\",\"shell.execute_reply\":\"2023-02-11T02:06:57.154843Z\"}}\nprediction_df = pd.DataFrame(prediction, columns=['y_hat']) # Creating a dataframe to evaluate the performance\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:06:57.157416Z\",\"iopub.execute_input\":\"2023-02-11T02:06:57.157846Z\",\"iopub.status.idle\":\"2023-02-11T02:06:57.167510Z\",\"shell.execute_reply.started\":\"2023-02-11T02:06:57.157811Z\",\"shell.execute_reply\":\"2023-02-11T02:06:57.166622Z\"}}\nprediction_df['close'] = actual\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:06:57.170657Z\",\"iopub.execute_input\":\"2023-02-11T02:06:57.170918Z\",\"iopub.status.idle\":\"2023-02-11T02:06:57.519440Z\",\"shell.execute_reply.started\":\"2023-02-11T02:06:57.170894Z\",\"shell.execute_reply\":\"2023-02-11T02:06:57.518520Z\"}}\nplt.figure(figsize=(16,16))\nsns.scatterplot(data=prediction_df, x=prediction_df['y_hat'], y=prediction_df['close'])\nplt.legend()\nplt.show()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-02-11T02:06:57.521051Z\",\"iopub.execute_input\":\"2023-02-11T02:06:57.521729Z\",\"iopub.status.idle\":\"2023-02-11T02:06:59.462856Z\",\"shell.execute_reply.started\":\"2023-02-11T02:06:57.521691Z\",\"shell.execute_reply\":\"2023-02-11T02:06:59.461902Z\"}}\nscore = model.evaluate(x_test, y_test, verbose=1)\nprint('Test loss:', score[0])\nprint('MAPE:', round(score[1],2), '%')","metadata":{"_uuid":"c35847bb-7929-4cb5-a44b-c5897b5e7191","_cell_guid":"07f0629e-f27a-4434-ae51-cef95b220c1b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}